from openai import OpenAI
import os
import re
import torch
from diffusers import StableDiffusion3Pipeline

llm_prompt_path = "llm_prompt.txt"

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", 'your_api_key_here'))

pipe = StableDiffusion3Pipeline.from_pretrained("stabilityai/stable-diffusion-3-medium-diffusers", torch_dtype=torch.float16)
pipe.to("cuda")

def img_generation(prompt, save_dir='temp'):
    '''
    This function generates an image based on the prompt and saves it to the specified directory
    
    Input: input prompt for image generation
    Output: path to the saved image
    
    Parameters:
    - prompt: input prompt for image generation
    - save_dir: directory to save the generated image
    '''
    global pipe
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    image = pipe(prompt).sample()
    save_path = os.path.join(save_dir, f"generated_image_{len(os.listdir(save_dir)) + 1}.png")

    image.save(save_path)
    print(f"Image saved to: {save_path}")
    return save_path

def recurrentStoryboard(current_clip, music_summary, previous_summary, previous_frame, previous_suggestions):
    '''
    This function generates a storyboard for a piece of music to express the music based on the input parameters
    
    Input: input parameters for generating the storyboard
    Output: generated storyboard
    
    Parameters:
    - current_clip: description of the current music clip
    - music_summary: summary of the overall music
    - previous_summary: summary of the previous story
    - previous_frame: description of the frame for each iteration and it will be used as input for the next iteration
    - previous_suggestions: optional suggestions for the previous frame
    '''
    prompt_file = open(llm_prompt_path, "r")
    template = prompt_file.read()
    try:
        # Generate completion from OpenAI's chat model
        completion = client.chat.completions.create(
            model="gpt-4o",
            temperature=0.0,
            timeout=100,
            messages=[
                {"role": "user", "content": template.format(current_clip=current_clip, music_summary=music_summary, previous_story=previous_summary, previous_frame=previous_frame, previous_suggestions=previous_suggestions)},
            ]
        )
        return completion.choices[0].message.content  # Return the predicted result
    except Exception as e:
        return "Error: " + str(e)
    
def extract_info(response):
    '''
    This function extracts the relevant information from the response generated by the recurrentStoryboard function
    
    Input: response generated by the recurrentStoryboard function
    Output: extracted information from the response
    
    Parameters:
    - response: response generated by the recurrentStoryboard function
    '''
    music_summary = re.search(r"## Summary of Overall Music ##\n(.*?)\n", response, re.DOTALL).group(1)
    previous_summary = re.search(r"## Updated Story Theme Summary ##\n(.*?)\n", response, re.DOTALL).group(1)
    previous_frame = re.search(r"## Description of Current Frame ##\n(.*?)\n", response, re.DOTALL).group(1)
    previous_suggestions = re.search(r"## Suggestions for the Next Frame ##\n(.*?)\n", response, re.DOTALL).group(1)
    return music_summary, previous_summary, previous_frame, previous_suggestions

#TODO: Implement get_current_clip with Music Processing
def get_current_clip():
    current_clip =  "A description of 10-second clip of the current music"
    return current_clip

def main():
    current_clip = get_current_clip()
    music_summary = current_clip
    previous_summary = ""
    previous_frame = ""
    previous_suggestions = ""
    while current_clip:
        response = recurrentStoryboard(current_clip, music_summary, previous_summary, previous_frame, previous_suggestions)
        print(response)
        music_summary, previous_summary, previous_frame, previous_suggestions = extract_info(response)
        img_generation(previous_frame)
        current_clip = get_current_clip()
    
