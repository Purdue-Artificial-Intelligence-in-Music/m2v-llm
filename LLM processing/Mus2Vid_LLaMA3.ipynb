{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQDp0sXX3qE4"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6rwbyFa5LkF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"GPU not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gX4PskL6UJP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# using 404Dataset\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Mus2VidLLaMA3\"\n",
        "AUTHOR = \"Mus2Vid\"\n",
        "\n",
        "with open(\"404Dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "with open(\"404Dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIfzFgLsm2kS"
      },
      "outputs": [],
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psywJyo75vt6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # Perform supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Use 4-bit quantized Llama-3-8b-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo,alpaca_zh_demo\",      # Use alpaca and self-awareness datasets\n",
        "  template=\"llama3\",                     # Use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # Use LoRA adapter to save GPU memory\n",
        "  lora_target=\"all\",                     # Add LoRA adapter to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # Path to save LoRA adapter\n",
        "  per_device_train_batch_size=2,               # Batch size\n",
        "  gradient_accumulation_steps=4,               # Gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # Use cosine learning rate decay\n",
        "  logging_steps=10,                      # Output a log every 10 steps\n",
        "  warmup_ratio=0.1,                      # Use learning rate warmup\n",
        "  save_steps=1000,                      # Save a checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # Learning rate\n",
        "  num_train_epochs=3.0,                    # Number of training epochs\n",
        "  max_samples=300,                      # Use 300 samples from each dataset\n",
        "  max_grad_norm=1.0,                     # Clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                   # Use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # Use float16 mixed precision training\n",
        "  use_liger_kernel=True,                   # Use Liger Kernel to accelerate training\n",
        ")\n",
        "\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbFsAE-y5so4"
      },
      "outputs": [],
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Use 4-bit quantized Llama-3-8b-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # Load previously saved LoRA adapter\n",
        "  template=\"llama3\",                     # Consistent with training\n",
        "  finetuning_type=\"lora\",                  # Consistent with training\n",
        "  quantization_bit=4,                    # Load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Use `clear` to clear conversation history, use `exit` to exit the program.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"Conversation history cleared\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA2kyAz-hXbp"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eERYoAOrhpcu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # Use non-quantized official Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # Load previously saved LoRA adapter\n",
        "  template=\"llama3\",                     # Consistent with training\n",
        "  finetuning_type=\"lora\",                  # Consistent with training\n",
        "  export_dir=\"llama3_lora_merged\",              # Directory to save the merged model\n",
        "  export_size=2,                       # Size of each weight file for the merged model (in GB)\n",
        "  export_device=\"cpu\",                    # Device used for merging model: `cpu` or `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # HuggingFace model ID for uploading the model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
